<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Memory-Augmented Diffusion for Minecraft-Style Worlds</title>
  <style type="text/css">
    :root {
      --main-title-size: 36px;
      --section-title-size: 24px;
      --subsection-title-size: 20px;
      --primary-color: #26C6DA;
      --primary-hover: #00ACC1;
      --bg-color: #121212;
      --content-bg: #1E1E1E;
      --border-color: #333;
      --text-color: #E0E0E0;
      --caption-color: #BBB;
      --sidebar-width: 220px;
    }
    
    body {
      background-color: var(--bg-color);
      color: var(--text-color);
      margin: 0;
      padding: 0;
      font-family: 'Segoe UI', Helvetica, Arial, sans-serif;
      line-height: 1.6;
    }
    
    .page-layout {
      display: flex;
      min-height: 100vh;
    }
    
    .sidebar {
      width: var(--sidebar-width);
      background-color: #1A1A1A;
      position: fixed;
      height: 100vh;
      overflow-y: auto;
      padding: 30px 20px;
      box-shadow: 2px 0 10px rgba(0,0,0,0.3);
      z-index: 100;
    }
    
    .sidebar-title {
      color: var(--primary-color);
      font-size: 18px;
      font-weight: bold;
      margin-bottom: 20px;
      padding-bottom: 10px;
      border-bottom: 1px solid var(--border-color);
    }
    
    .sidebar ul {
      padding: 0.01;
      margin: 0;
    }

    .sidebar ul ul {
      margin-top: 0px;
      margin-left: 0px;
    }

    .sidebar ul ul li {
      list-style-type: none;
      margin-bottom: 0px;
      font-size: 0.9em;
    }
    
    .sidebar li {
      list-style-type: none;
      margin-bottom: 0px;
    }
    
    .sidebar a {
      color: var(--text-color);
      text-decoration: none;
      display: block;
      padding: 8px 10px;
      border-radius: 4px;
      transition: all 0.2s ease;
    }
    
    .sidebar a:hover {
      background-color: rgba(38, 198, 218, 0.1);
      color: var(--primary-color);
      transform: translateX(5px);
    }
    
    .main-content {
      flex: 1;
      margin-left: var(--sidebar-width);
      padding: 40px;
    }
    
    .container {
      max-width: 1100px;
      margin: 0 auto;
    }
    
    .content {
      background: var(--content-bg);
      border-radius: 8px;
      padding: 40px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.2);
    }
    
    p { 
      line-height: 1.7;
      margin-bottom: 20px;
    }
    
    img, video { 
      max-width: 100%;
      display: block;
      margin: 0 auto;
      border-radius: 6px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.15);
    }
    
    a { 
      color: var(--primary-color);
      text-decoration: none;
      transition: color 0.2s ease;
    }
    
    a:hover { 
      color: var(--primary-hover);
    }
    
    .main-title { 
      font-size: var(--main-title-size);
      margin: 0 0 16px 0;
      color: var(--primary-color);
      font-family: 'Courier New', Courier, monospace;
      letter-spacing: -0.5px;
      border-bottom: 2px solid var(--primary-color);
      padding-bottom: 10px;
      display: inline-block;
    }
    
    h2 {
      font-size: var(--section-title-size);
      margin-top: 40px;
      margin-bottom: 20px;
      color: var(--primary-color);
      border-left: 4px solid var(--primary-color);
      padding-left: 12px;
    }
    
    h3 {
      font-size: var(--subsection-title-size);
      margin-top: 30px;
      margin-bottom: 15px;
      color: var(--primary-color);
    }
    
    figure { 
      margin: 30px 0;
      text-align: center;
    }
    
    figcaption {
      font-size: 0.9em;
      color: var(--caption-color);
      margin-top: 10px;
      font-style: italic;
    }
    
    hr { 
      height: 1px;
      border: none;
      background-color: var(--border-color);
      margin: 40px 0;
    }
    
    .citation { 
      font-size: 0.9em;
      background-color: #252525;
      padding: 15px;
      border-radius: 6px;
      border-left: 3px solid var(--primary-color);
      margin-top: 30px;
    }
    
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 25px 0;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
      overflow: hidden;
      border-radius: 6px;
    }
    
    th, td {
      border: 1px solid var(--border-color);
      padding: 12px;
      text-align: center;
    }
    
    th { 
      background: #2A2A2A;
      color: var(--primary-color);
      font-weight: bold;
    }
    
    tr:hover {
      background-color: rgba(38, 198, 218, 0.05);
    }
    
    .image-row {
      display: flex;
      justify-content: space-between;
      gap: 15px;
      margin: 30px 0;
    }
    
    .image-row img {
      flex: 1;
      object-fit: cover;
      max-height: 250px;
      width: auto;
    }
    
    .video-container {
      margin: 30px 0;
    }
    
    .video-container video {
      width: 100%;
      max-height: 400px;
    }
    
    .author-info {
      display: flex;
      align-items: center;
      margin-bottom: 30px;
    }
    
    .author-info p {
      margin: 0 20px 0 0;
    }
    
    .references {
      background: #252525;
      border-left: 3px solid var(--primary-color);
      padding: 20px;
      border-radius: 6px;
      margin-top: 40px;
    }
    
    .references ol {
      margin: 10px 0 0 0;
      padding-left: 1.5em;
    }
    
    .references li {
      margin-bottom: 10px;
      line-height: 1.5;
    }
    
    strong {
      color: #fff;
    }
    
    em {
      color: var(--primary-color);
      font-style: italic;
    }
    
    .image-comparison {
      display: flex;
      justify-content: space-between;
      gap: 20px;
      margin: 30px 0;
    }
    
    .image-comparison figure {
      flex: 1;
      margin: 0;
    }
    
    .video-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 20px;
      margin: 30px 0;
    }
    
    .video-item {
      display: flex;
      flex-direction: column;
    }
    
    .video-item video {
      width: 100%;
      max-height: 300px;
      object-fit: cover;
    }
    
    .video-item figcaption {
      margin-top: 8px;
      text-align: center;
    }
    
    @media (max-width: 768px) {
      .video-grid {
        grid-template-columns: 1fr;
      }
    }
    
    @media (max-width: 1024px) {
      .sidebar {
        width: 200px;
      }
      
      .main-content {
        margin-left: 200px;
      }
    }
    
    @media (max-width: 768px) {
      .page-layout {
        flex-direction: column;
      }
      
      .sidebar {
        width: 100%;
        height: auto;
        position: relative;
        padding: 15px;
      }
      
      .main-content {
        margin-left: 0;
        padding: 20px;
      }
      
      .image-row, .image-comparison {
        flex-direction: column;
      }
      
      .image-row img, .image-comparison figure {
        max-height: none;
      }
      
      .video-grid {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>
<body>
  <div class="page-layout">
    <!-- SIDEBAR -->
    <div class="sidebar">
      <div class="sidebar-title">Contents</div>
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#background">Background &amp; Related Work</a></li>
        <li>
          <a href="#methods">Methods &amp; Experiments</a>
          <ul>
            <li><a href="#method1">Method 1</a></li>
            <li><a href="#method2">Method 2</a></li>
          </ul>
        </li>
        <li>
          <a href="#results">Results &amp; Analysis</a>
          <ul>
            <li><a href="#results1">Results: Method 1</a></li>
            <li><a href="#results2">Results: Method 2</a></li>
          </ul>
        </li>
        <li><a href="#conclusion">Conclusion &amp; Discussion</a></li>
        <li><a href="#references">References</a></li>
      </ul>
    </div>

    <!-- MAIN CONTENT -->
    <div class="main-content">
      <div class="container">
        <div class="content">
          <!-- HEADER -->
          <h1 class="main-title">Memory-Augmented Diffusion for Minecraft-Style Worlds</h1>
          <div class="author-info">
            <p><strong>Co-Authors:</strong> Arthur De Los Santos, Joachim Asare</p>
            <p><strong>Course:</strong> 6.8300 Advances in Computer Vision, Spring 2025</p>
          </div>
          <hr>

          <!-- INTRO -->
          <section id="introduction">
            <h2>Introduction</h2>
            <p>
              Generative diffusion models can produce rich and visually compelling scenes,
              but they often lack <em>structural memory</em>: when the viewpoint shifts,
              previously generated content may be forgotten or reimagined, leading to
              inconsistencies. This limitation is especially problematic in 3D settings,
              where maintaining coherent geometry across frames is crucial for realism.
              Our project explores how to anchor diffusion models to an explicit scene
              representation so that worlds remain consistent as the camera moves.
            </p>
          
            <p>
              To investigate this challenge, we implemented and evaluated
              <strong>two complementary pipelines</strong> for generating Minecraft-style
              voxel scenes:
              <ul>
                <li><strong>Method 1</strong> begins with a small number of
                    sparse RGB-D observations and incrementally hallucinated outward using
                    a memory-augmented diffusion model. The system enforces consistency
                    across canonical and held-out views by aligning output with an
                    accumulated mesh.</li>
                <li><strong>Method 2</strong> procedurally generates full
                    synthetic voxel scenes and captures them from 100 orbiting viewpoints.
                    These RGB-D frames are fused into a TSDF volume (a voxel grid) that then guides
                    diffusion during image generation. Comparisons are made against
                    baseline “control” runs with no memory guidance.</li>
              </ul>
              While both methods share the same core diffusion backbone and voxel-centric
              priors, they differ in how memory is introduced (sparse → complete) and
              when consistency is enforced (post hoc in Method 1, real-time in Method 2).
            </p>
          
            <p>
              We chose Minecraft-style scenes for their ease of generation, visual
              regularity, and computational efficiency. Unlike complex real-world 3D
              environments—which require high-resolution assets, accurate lighting, and
              powerful GPUs—Minecraft scenes can be rendered as lightweight block meshes
              with simple color cues. This makes them ideal for rapid prototyping on
              limited hardware (e.g., a Laptop CPU/GPU) while still capturing key
              challenges of scene memory, multi-view consistency, and depth-aware
              generation.
            </p>
          
            <p>
              Below, we present the architectural details and results of both methods,
              including quantitative metrics and qualitative walkthroughs. We highlight
              the unique strengths of each pipeline, compare their behavior under similar
              prompts and poses, and discuss trade-offs between hallucination quality,
              geometric fidelity, and temporal coherence.
            </p>
          </section>

          <!-- BACKGROUND -->
          <section id="background">
            <h2>Background &amp; Related Work</h2>
          
            <p>
              Generative scene modeling has seen rapid progress through two major paradigms: implicit 3D representations (e.g., NeRF) and 2D generative diffusion models. While NeRF-based methods inherently maintain multi-view consistency by modeling a continuous volumetric function, they require per-scene optimization and struggle to generalize. Conversely, diffusion models offer expressive and general-purpose generation, but lack structural persistence and often hallucinate inconsistent content across views. Our work combines insights from both approaches by conditioning a diffusion model on an explicit memory: a voxel-based representation that persists across frames and enforces geometric structure.
            </p>
          
            <p>
              <strong>Neural Radiance Fields (NeRF)</strong> represent a scene as a 5D function (position and viewing direction mapped to color and density), achieving photorealistic novel-view synthesis. While NeRF inherently provides scene memory and geometric consistency, it is slow to train, scene-specific, and typically static. Extensions like NeRF-VAE introduce latent variables to generalize across scenes and encode global memory, enabling generation of novel views from sparse input. These models motivate our use of geometry as a persistent backbone but are less suitable for real-time or creative generation tasks without retraining.
            </p>
          
            <p>
              <strong>3D-aware generative models</strong> like GIRAFFE and π-GAN further highlight the value of compositional structure. GIRAFFE models a scene as a set of independent objects in a 3D feature space, enabling more disentangled and consistent synthesis across viewpoints. These methods demonstrate that enforcing 3D priors—even implicitly—helps stabilize content under transformation. However, they typically operate on synthetic datasets and require training from scratch. Our approach instead leverages pretrained diffusion priors guided by explicit voxel memory to produce similar view-consistent effects without specialized training.
            </p>
          
            <p>
              <strong>Conditioned diffusion models</strong> offer a middle ground: models like Zero-1-to-3 and ControlNet fine-tune diffusion to accept structural guidance such as depth maps or poses. In particular, ControlNet enables conditioning on arbitrary structural inputs like segmentation masks or monocular depth, making it suitable for applications where scene geometry is available but incomplete. Our project builds on this idea: we use a voxel grid (constructed from past observations or synthetic renderings) to produce depth hints at each frame, which then guide a Stable Diffusion model to generate RGB images consistent with the underlying 3D scene.
            </p>
          
            <p>
              In summary, our work draws from the strengths of implicit 3D representations and 2D generative models. By maintaining a persistent volumetric memory and conditioning a depth-to-image diffusion model on it, we achieve improved consistency and scene persistence without needing to retrain from scratch. Our approach adapts well to resource-constrained environments and is particularly well-suited to stylized voxel worlds like Minecraft, where structural alignment and geometric recall are crucial for believable world synthesis.
            </p>
          </section>


          <!-- METHODS -->
          <section id="methods">
            <h2>Methods &amp; Experiments</h2>
            <h3 id="method1">Method 1</h3>
            <h4>Phase 1: Voxel Memory Initialization</h4>
            <p>
              Eight ground‐truth RGB‐D captures of a 16³ random block world are fused into a 256³ voxel grid (voxel size 0.125 m). We back-project each pixel to update occupancy and color.
            </p>
            <div class="image-row">
              <img src="images/phase1_depth.png" alt="GT Depth Map">
              <img src="images/phase1_depth_hint.png" alt="Voxel Depth Hint">
              <img src="images/phase1_rgb.png" alt="Fused RGB">
            </div>
            <figcaption>
              Figure 1: (Left) ground-truth depth, (center) first-pass depth hints from voxel memory, (right) fused RGB silhouette after eight views.
            </figcaption>

            <h4>Phase 2: Mesh Extraction</h4>
            <p>
              We threshold occupancy (count > 0) and run Marching Cubes to extract a colored triangle mesh of ~78k vertices.
            </p>
            <figure>
              <img src="images/phase2_mesh.png" alt="Phase 2 Mesh" style="max-height: 400px;">
              <figcaption>
                Figure 2: Raw mesh after Phase 2 Marching Cubes.
              </figcaption>
            </figure>

            <h4>Phase 3: Diffusion & Consistency</h4>
            <p>
              For each of the four canonical views, we ray-march to obtain depth hints, feed them into ControlNet (Stable Diffusion) with a "Minecraft-style voxel art" prompt, fuse the hallucinated RGB-D, and extract a mesh (Phase 3 Diffusion). We then re-render held-out angles, hallucinate, and re-fuse to enforce multi-view consistency (Phase 3 Consistency).
            </p>
            <figure>
              <img src="images/phase3_diffusion_mesh.png" alt="Phase 3 Diffusion Mesh" style="max-height: 400px;">
              <figcaption>
                Figure 3: Mesh after diffusion-based texture hallucination (Phase 3 Diffusion). Notice increased coverage but visible shape artifacts (gaps and spikes).
              </figcaption>
            </figure>

            <h5>Interactive Walkthroughs</h5>
            <div class="video-grid">
              <div class="video-item">
                <video src="images/phase1.mp4" autoplay loop muted playsinline></video>
                <figcaption>Video 1: Phase 1 fusion.</figcaption>
              </div>
              <div class="video-item">
                <video src="images/Phase2.mp4" autoplay loop muted playsinline></video>
                <figcaption>Video 2: Phase 2 mesh extraction.</figcaption>
              </div>
              <div class="video-item">
                <video src="images/phase3_diffusion.mp4" autoplay loop muted playsinline></video>
                <figcaption>Video 3: Phase 3 diffusion fusion.</figcaption>
              </div>
              <div class="video-item">
                <video src="images/phase3_consistency.mp4" autoplay loop muted playsinline></video>
                <figcaption>Video 4: Phase 3 multi-view consistency.</figcaption>
              </div>
            </div>

          <h3 id="method2">Method 2</h3>

          <h4>Phase 1: Scene Data Generation (Raw World)</h4>
          <p>
            Each synthetic Minecraft-style world is procedurally generated, rendered from 200 orbiting viewpoints, and stored with RGB-D, extrinsic, and intrinsic information.
          </p>

          <div class="video-grid">
            <div class="video-item">
              <video src="images/arthur/raw/scene1.mp4" autoplay loop muted playsinline></video>
              <figcaption>Raw Scene 1</figcaption>
            </div>
            <div class="video-item">
              <video src="images/arthur/raw/scene2.mp4" autoplay loop muted playsinline></video>
              <figcaption>Raw Scene 2</figcaption>
            </div>
            <div class="video-item">
              <video src="images/arthur/raw/scene3.mp4" autoplay loop muted playsinline></video>
              <figcaption>Raw Scene 3</figcaption>
            </div>
            <!-- <div class="video-item">
              <video src="images/arthur/control/scene1.mp4" autoplay loop muted playsinline></video>
              <figcaption>Control Diffusion (no RGB-D) Scene 1</figcaption>
            </div> -->
          </div>

          <h4>Phase 2: Scene Representation (TSDF Voxel Grid)</h4>
          <p>
            Of the 200 RGB-D frames, 100 fuse into a scalable TSDF (voxel size <span style="white-space:nowrap;">v<sub>s</sub></span>,
            truncation <span style="white-space:nowrap;">τ</span> per Table 1) using open3d's integration function.
            This volume serves two roles: (i) produces geometry-aware depth hints;
            (ii) maintains persistent memory for multi-view diffusion generation.
          </p>

          <h4>Phase 3: Memory-Augmented Diffusion</h4>
          <p>
            Given the live TSDF, we render an RGB frame <code>R<sub>mem</sub></code> and a
            metric depth map <code>D<sub>mem</sub></code> for the next camera pose
            (intrinsic, extrinsic) for the remaining 100 frames of the scene.
            These tensors are then passed to
            <code>StableDiffusionDepth2Img</code> together with the text prompt. Results from the diffusion
            model are then re-integrated into the scene using the generated RGB and a monocular depth estimator Depth-Anything-V2. 
            Hyper-parameters (<em>strength, guidance, steps, voxel size, truncation</em>)
            are varied across nine test scenes (Table 1).  A <code>--control</code> flag
            disables RGB-D conditioning, producing a baseline “guess” of what diffusion
            alone would generate.
          </p>

          <table>
            <tr><th>Scene</th><th>str.</th><th>guid.</th><th>v<sub>s</sub></th><th>τ</th><th>steps</th></tr>
            <tr><td>1</td><td>0.25</td><td>7.5</td><td>0.10</td><td>0.30</td><td>30</td></tr>
            <tr><td>2</td><td>0.50</td><td>7.5</td><td>0.10</td><td>0.30</td><td>30</td></tr>
            <tr><td>3</td><td>0.75</td><td>7.5</td><td>0.10</td><td>0.30</td><td>30</td></tr>
            <tr><td>4</td><td>0.50</td><td>7.5</td><td>0.06</td><td>0.20</td><td>30</td></tr>
            <tr><td>5</td><td>0.50</td><td>7.5</td><td>0.20</td><td>0.60</td><td>30</td></tr>
            <tr><td>6</td><td>0.50</td><td>15</td><td>0.10</td><td>0.30</td><td>30</td></tr>
            <tr><td>7</td><td>0.50</td><td>3</td><td>0.10</td><td>0.30</td><td>30</td></tr>
            <tr><td>8</td><td>0.50</td><td>7.5</td><td>0.10</td><td>0.30</td><td>60</td></tr>
            <tr><td>9</td><td>0.50</td><td>7.5</td><td>0.10</td><td>0.30</td><td>15</td></tr>
            <caption style="caption-side:bottom; margin-bottom:8px;"><strong>Table 1: Per-scene hyper-parameters</strong></caption>
          </table>

          <div class="image-comparison">
            <figure>
              <video src="images/arthur/raw/scene2.mp4"  autoplay loop muted playsinline></video>
              <figcaption>Raw orbit (ground-truth RGB-D)</figcaption>
            </figure>
            <figure>
              <video src="images/arthur/control/scene2.mp4" autoplay loop muted playsinline></video>
              <figcaption>Control diffusion (no memory)</figcaption>
            </figure>
            <figure>
              <video src="images/arthur/generated/scene2.mp4" autoplay loop muted playsinline></video>
              <figcaption>Memory-conditioned result</figcaption>
            </figure>
          </div>
          <figcaption>
            Figure N: For <strong>Scene 2</strong> (strength 0.5, voxel 0.10 m, truncation 0.30 m) the
            memory-conditioned run preserves the hill silhouette and tree trunks, whereas
            the control run completely reshuffles block colours and heights.
          </figcaption>

          </section>

          <!-- RESULTS -->
          <section id="results">
            <h2>Results &amp; Analysis</h2>
            <h3 id="results1">Results: Method 1</h3>
            <h4>Phase 1 Coverage & Depth Error</h4>
            <p><strong>Phase 1 – Baseline Diffusion (No Memory):</strong> In the first phase, the diffusion model was run with basic conditioning (an initial depth hint) but without any persistent memory of the scene. We found that while the model can produce photorealistic frames from a single viewpoint, it struggles with consistency across views. As the camera moves, the diffusion model often <em>hallucinates</em> new structures or forgets previously seen ones. For example, when the camera rotates to reveal unseen areas, the model has no knowledge of those regions and fails to generate coherent content there. We provided the model with a depth hint from the initial view via ControlNet, but once the viewpoint changed, this hint became misaligned with the true scene geometry. The <em>Depth Hint Absolute Error</em> heatmap in Figure 4 illustrates this issue: areas in bright colors indicate large discrepancies (up to ~25 meters) between the provided depth hint and the ground-truth depth after the camera moved. These large errors confirm that a single-view hint is insufficient for novel view synthesis – the diffusion model was essentially guessing content for regions outside the initial view, leading to inconsistency.</p>
            
            <table>
              <tr><th>View</th><th>Coverage (%)</th></tr>
              <tr><td>1</td><td>10.00</td></tr>
              <tr><td>2</td><td>21.07</td></tr>
              <tr><td>3</td><td>31.52</td></tr>
              <tr><td>4</td><td>43.81</td></tr>
              <tr><td>5</td><td>54.47</td></tr>
              <tr><td>6</td><td>67.29</td></tr>
              <tr><td>7</td><td>78.29</td></tr>
              <tr><td>8</td><td>91.18</td></tr>
            </table>
            
            <p>
              Coverage increases nearly linearly as more viewpoints are fused, reaching 91% by view 8. However, only ~32% of pixels have valid depth hints (mean absolute error 12.61 m, max 29.31 m), indicating imprecise geometry for distant blocks.
            </p>
            
            <figure>
              <img src="images/phase1_error.png" alt="Phase 1 Error Heatmap" style="max-width: 80%; max-height: 350px;">
              <figcaption>
                Figure 4: Absolute depth error where hints exist. High errors (orange) occur at farthest surfaces. This heatmap shows where the depth hint provided to ControlNet is very inaccurate when the viewpoint changes, leading the diffusion model to hallucinate or distort scene content in those areas.
              </figcaption>
            </figure>
            
            <p>This failure mode is evident in the rendered video as well. The clip above (Video 1) shows a short camera pan with the Phase 1 baseline. As the view changes and returns, objects do not remain consistent – some blocks appear, disappear, or change color/shape. Notably, when the camera looks behind initial structures, the model does not render the back faces at all (they often appear as black or random textures), demonstrating <strong>zero novel-view coverage</strong> for surfaces that were never observed. By the time the camera returns to the original angle, the scene has drifted: the final view deviates from the starting view, highlighting poor temporal consistency.</p>

            <h4>Phase 2 – Integrating Spatial Memory</h4>
            <p><strong>Phase 2 – Integrating Spatial Memory (Partial Reconstruction):</strong> In Phase 2, we introduced a structured voxel grid as a persistent scene memory, akin to the approach of KinectFusion, to integrate depth observations over time. As the camera moved, depth from each new view was fused into this voxel representation (using truncated signed distance functions for surface mapping, similar to KinectFusion). This allowed the system to accumulate a partial 3D model of the scene in memory. The diffusion model could then be conditioned not just on the current view's hint, but also on the memory-derived depth for known regions. The impact was immediately visible: previously seen surfaces stayed much more consistent in appearance and position, since the model was now guided by the stored geometry.</p>
            
            <p>Figure 2 shows the extracted mesh from the voxel memory after a limited camera sweep in Phase 2. Only surfaces that the camera has observed are present; they appear as isolated black fragments corresponding to the front faces of the Minecraft-style blocks. This partial reconstruction demonstrates one strength of Phase 2: the memory successfully preserves seen content. However, the limitations are also clear – large portions of the scene remain missing (e.g., the hidden faces and any areas never viewed). In other words, Phase 2's memory is accurate but incomplete. The system does not yet attempt to generate new geometry for unseen regions; it simply leaves them blank.</p>

            <h4>Phase 3 – Memory-Augmented Diffusion</h4>
            <p><strong>Phase 3 – Memory-Augmented Diffusion (Hallucination of Unseen Regions):</strong> In Phase 3, we leveraged the accumulated memory to actively guide the diffusion model in filling out the scene, including regions not directly observed. Here we introduced two variants: <em>Phase 3 Diffusion</em> and <em>Phase 3 Consistency</em>. In the Phase 3 Diffusion stage, the model was conditioned on both the current view's known depth and the global memory structure. Crucially, if the memory indicated empty or unknown space (for example, the back of a block that hadn't been seen), the diffusion model was encouraged to hallucinate plausible content to occupy that space.</p>
            
            <p>Figure 3 depicts the mesh after Phase 3 Diffusion. We see a much denser, more complete set of surfaces (rendered in gray) compared to Phase 2's sparse black fragments. The model has hallucinated the backs and even some unseen internal parts of the scene, guided by the memory's structure. This dramatically increased the <strong>coverage</strong> of the scene: the fraction of ground-truth surfaces that have a corresponding generated surface went up substantially. However, these hallucinated surfaces are not guaranteed to be geometrically accurate – they are the model's best guess.</p>

            <h4>Phase 3 – Enforcing Multi-View Consistency</h4>
            <p><strong>Phase 3 – Enforcing Multi-View Consistency:</strong> After obtaining the hallucinated full scene from Phase 3 Diffusion, we applied an additional consistency refinement step (Phase 3 Consistency). The goal here was to ensure that the hallucinated content remains self-consistent when viewed from multiple angles, improving geometric fidelity. We achieved this by using the accumulated voxel memory as a constraint: any new view rendered must align with the memory's surfaces. This process is conceptually similar to approaches like DreamFusion or Scene Representation Transformer, which aim for a consistent underlying scene representation that can render from any angle.</p>
            
            <p>This consistency enforcement did have a side effect: it sometimes reduced the visual detail on those surfaces because the diffusion model had less freedom to alter them. The images became slightly more blurred or simple on consistent surfaces, as the model essentially had to obey the memory. Thus, we observe a classic quality vs. consistency trade-off: Phase 3 Diffusion produced the most visually detailed hallucinations, whereas Phase 3 Consistency prioritized ensuring those features didn't contradict across views.</p>

            <h4>Quantitative Evaluation</h4>
            <p><strong>Quantitative Evaluation:</strong> We evaluated our approach using several metrics. First, we measured <em>coverage</em>, defined as the percentage of ground-truth surface area that our generated outputs successfully reconstruct or cover in novel views. Second, we computed the <em>mean absolute error (MAE)</em> in depth for the generated surfaces, comparing the model's output depth to the ground-truth depth map at corresponding pixels. We also tracked image-based metrics (PSNR and SSIM) for frames where ground-truth images were available.</p>
            
            <div class="image-comparison">
              <figure>
                <img src="images/eval_phase3_diffusion.png" alt="Phase 3 Diffusion Eval">
                <figcaption>Figure 5a: Phase 3 Diffusion Evaluation</figcaption>
              </figure>
              <figure>
                <img src="images/eval_phase3_consistency.png" alt="Phase 3 Consistency Eval">
                <figcaption>Figure 5b: Phase 3 Consistency Evaluation</figcaption>
              </figure>
            </div>
            <figcaption>
              Figure 5: Quantitative evaluation of multi-view reconstruction for Phase 3. (a) <em>Phase 3 Diffusion Eval:</em> The green line shows coverage percentage as novel view angle increases; coverage drops from ~35% at 50° to 0% by 150°. The blue square indicates depth MAE at 50° (~14.8 m). (b) <em>Phase 3 Consistency Eval:</em> With consistency enforcement, coverage at 50° is slightly lower (~28%) but depth MAE improves to ~14.2 m, reflecting more accurate geometry.
            </figcaption>
            
            <p>The plots in Figure 5 summarize the multi-view consistency evaluation for Phase 3, comparing the Diffusion-only vs. Consistency-refined approaches. For Phase 3 Diffusion (Figure 5a), at a moderate viewpoint shift (~50°), coverage is about 35% – meaning the model manages to render roughly one-third of the true surfaces in that novel view. However, as the novel view angle grows (turning the camera to see the backside of the scene ~150° or more), coverage plummets to 0%.</p>
            
            <p>In contrast, for Phase 3 Consistency (Figure 5b), the coverage at 50° is a bit lower (~28%). This slight drop is because the consistency method is conservative – it avoids counting surfaces unless they are well-supported by memory. However, the depth MAE for those that are rendered is lower (around 14.2 m at 50°), suggesting that Phase 3 Consistency's outputs, though covering slightly less area, are more geometrically accurate.</p>

            <h4>Hallucinated RGB Quality</h4>
            <figure>
              <img src="images/quality_results.png" alt="Quality Metrics" style="max-width: 80%; max-height: 350px;">
              <figcaption>
                Figure 6: PSNR & SSIM vs. held-out angle. High fidelity near seen views (30°, 120°) but plummets elsewhere (60°, 150°).
              </figcaption>
            </figure>
            <p>
              Average PSNR≈8.9 dB and SSIM≈0.43 reflect reasonable local texture plausibility but poor global consistency. Failures at 60°/150° align with zero coverage—no geometry to paint on.
            </p>

            <h3 id="results2">Results: Method 2</h3>

            <p>
              The grid below shows the <em>final</em> memory-conditioned videos for all nine
              scenes.  Qualitatively we observe three trends:
              (i) increased <strong>strength</strong> (0.25 → 0.75) mixes more prompt colour
              into the world at the expense of block shape fidelity;
              (ii) halving the voxel size (Scene 4, 0.06 m) comapred to doubling the voxel size (Scene 5, 0.20 m) leads to less scene 'breaking' as the diffusion model continues to generate;
              (iii) low guidance (Scene 7, 3.0) preserves the scene structure more compared to high guidance (Scene 6, 15).
            </p>

            <div class="video-grid">
              <div class="video-item">
                <video src="images/arthur/generated/scene1.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;1</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene2.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;2</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene3.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;3</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene4.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;4</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene5.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;5</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene6.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;6</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene7.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;7</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene8.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;8</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/generated/scene9.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;9</figcaption>
              </div>
            </div>

            <p>
              In every case, the conditioned output is <em>visually closer</em> to the raw
              orbit than its control counterpart. However, when <code>strength&nbsp;&gt;&nbsp;0.75</code> the model tends
              to frequently erase and replace small foliage blocks, hinting at a trade-off between stylistic
              freedom and geometric recall similar to the Phase 3 Diffusion vs. Consistency contrast in Method 1.
            </p>
            
            <div class="video-grid">
              <div class="video-item">
                <video src="images/arthur/control/scene1.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;1</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/control/scene2.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;2</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/control/scene3.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;3</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/control/scene6.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;6</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/control/scene7.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;7</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/control/scene8.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;8</figcaption>
              </div>
              <div class="video-item">
                <video src="images/arthur/control/scene9.mp4" autoplay loop muted playsinline></video>
                <figcaption>Scene&nbsp;9</figcaption>
              </div>
            </div>
            
            <p>Note that scene 4 and 5 are excluded given that they change the voxel grid size, which is not part of the control. Also note that the results for this method are more qualitative by nature, necessitating the use of a qualitative analysis of the results</p>

            <hr>

          </section>

          <!-- CONCLUSION -->
          <section id="conclusion">
            <h2>Conclusion &amp; Discussion</h2>
          
            <p>
              This project explored how persistent 3D scene memory can be used to guide diffusion models toward more structurally coherent image generation. Across two complementary pipelines—one seeded from sparse real RGB-D inputs (Method 1) and another using procedurally generated Minecraft scenes (Method 2)—we investigated how memory affects hallucination quality, geometric fidelity, and consistency over camera movement.
            </p>
          
            <p>
              In <strong>Method 1</strong>, we demonstrated that fusing sparse RGB-D captures into a voxel memory enables the diffusion model to better preserve known content while hallucinating plausible completions for occluded or unobserved regions. By introducing multi-view consistency constraints in Phase 3, we showed that mesh quality and geometry coverage improve substantially compared to baseline diffusion. However, this approach still struggles with novel-view generalization: regions that were never observed (e.g. object backs) are often filled with poor guesses or remain missing. The trade-off between visual richness and geometric realism was especially clear in our quantitative metrics: Diffusion-only outputs had higher scene coverage, while Consistency-enforced results showed lower depth error but were more visually conservative.
            </p>
          
            <p>
              In <strong>Method 2</strong>, we shifted toward a full control pipeline using procedurally generated voxel worlds. This allowed us to perform large-scale qualitative comparisons across nine different hyperparameter configurations. We found that <em>diffusion strength</em> and <em>guidance scale</em> exerted the most visible influence: higher strength amplified prompt adherence but often destroyed small-scale block geometry, while lower guidance improved geometric retention at the cost of scene vibrancy. Scene 4 and 5 revealed the sensitivity of voxel resolution—smaller voxel sizes (0.06 m) led to better preservation of tree trunks and block silhouettes, while larger ones (0.20 m) broke scene integrity earlier in the generation process.
            </p>
          
            <p>
              A key difference between the methods lies in how memory is used. In Method 1, memory is accumulated <em>after</em> the fact, providing an opportunity to hallucinate beyond the initial bounds and then refine those guesses. In Method 2, the TSDF is constructed <em>before</em> generation and constrains diffusion in real time, making it more stable but less adaptive. As a result, Method 1 exhibits greater creativity in unobserved regions but also larger inconsistencies; Method 2 prioritizes temporal stability and accurate re-rendering of known geometry, but sometimes yields less expressive results.
            </p>
          
            <p>
              Both methods underscore a fundamental tension in generative scene synthesis: the more constraints we impose to preserve structure and consistency, the less freedom the model has to produce high-frequency or imaginative content. The ideal system likely lies somewhere in between—a hybrid pipeline that dynamically adjusts conditioning strength, enforces memory consistency softly rather than rigidly, and perhaps learns to trust or ignore parts of memory based on confidence.
            </p>
          
            <p>
              In conclusion, our work shows that integrating memory-aware 3D scene representations into diffusion workflows is not only feasible on modest hardware, but also highly beneficial for long-range visual coherence. Minecraft-style voxel environments proved to be a powerful abstraction for testing scene persistence under view shifts, offering a scalable sandbox for future research. Future extensions might include memory pruning, SLAM-based camera estimation, or learned TSDF confidence weighting. By uniting generative models with structured geometry, we take a step toward persistent, controllable, and spatially grounded visual world synthesis.
            </p>
          </section>

          <!-- REFERENCES -->
          <section id="references">
            <h2>References</h2>
            <div class="references">
              <ol>
                <li>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., &amp; Ng, R. (2020). <em>NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.</em> In <strong>ECCV 2020</strong>. (<a href="https://arxiv.org/abs/2003.08934">arXiv:2003.08934</a>)</li>
                <li>Newcombe, R. A., Izadi, S., Hilliges, O., Molyneaux, D., Kim, D., Davison, A. J., ... &amp; Fitzgibbon, A. (2011). <em>KinectFusion: Real-Time Dense Surface Mapping and Tracking with Kinect.</em> In <strong>ISMAR 2011</strong>. (<a href="https://doi.org/10.1109/ISMAR.2011.6092378">doi:10.1109/ISMAR.2011.6092378</a>)</li>
                <li>Zhang, L., Rao, A., &amp; Agrawala, M. (2023). <em>Adding Conditional Control to Text-to-Image Diffusion Models.</em> <strong>arXiv preprint</strong> arXiv:2302.05543. (<a href="https://arxiv.org/abs/2302.05543">link</a>)</li>
                <li>Fan, L., Liu, X., Li, Y., Xu, M., Wang, Y., Shen, Y., ... &amp; Song, D. (2022). <em>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge.</em> In <strong>NeurIPS 2022</strong>. (<a href="https://arxiv.org/abs/2206.08853">arXiv:2206.08853</a>)</li>
                <li>Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., &amp; Vondrick, C. (2023). <em>Zero-1-to-3: Zero-shot One Image to 3D Object.</em> To appear in <strong>ICCV 2023</strong>. (<a href="https://arxiv.org/abs/2303.11328">arXiv:2303.11328</a>)</li>
                <li>Poole, B., Jain, A., Barron, J. T., &amp; Mildenhall, B. (2022). <em>DreamFusion: Text-to-3D Using 2D Diffusion.</em> <strong>arXiv preprint</strong> arXiv:2209.14988.</li>
                <li>Niemeyer, M., &amp; Geiger, A. (2021). <em>GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields.</em> In <strong>CVPR 2021</strong>. (<a href="https://arxiv.org/abs/2011.12100">arXiv:2011.12100</a>)</li>
                <li>Sajjadi, M. S. M., Meyer, H., Pot, E., Bergmann, U., Greff, K., Radwan, N., ... &amp; Tagliasacchi, A. (2022). <em>Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations.</em> In <strong>CVPR 2022</strong>. (<a href="https://arxiv.org/abs/2111.13152">arXiv:2111.13152</a>)</li>
              </ol>
            </div>
          </section>
        </div>
      </div>
    </div>
  </div>
</body>
            <!-- <p>This failure mode is evident in the rendered video as well. The clip above (Video 1) shows a short camera pan with the Phase 1 baseline. As the view changes and returns, objects do not remain consistent – some blocks appear, disappear, or change color/shape. Notably, when the camera looks behind initial structures, the model does not render the back faces at all (they often appear as black -->
